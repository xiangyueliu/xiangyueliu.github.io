<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiangyue Liu</title>
  
  <meta name="author" content="Xiangyue Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css"> 
  <link rel="icon" type="image/svg" href="images/2b50.svg">

<style>
/* 	li{padding-top: 12px;} */
	li{padding-top: 8px;}
	gray{color: #6C6C6C}
</style>

	
<style>
  .gray-link:link, .gray-link:visited {
    color: gray;
    text-decoration: none;
  }
  .gray-link:hover {
    color: #87CEFA;
    text-decoration: underline;
  }
</style>



<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@600;700&display=swap" rel="stylesheet">
	
</head>	
	
<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">

           <td style="padding:2.5%;width:23%;max-width:23%">      
		<a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
		  
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
		      <link rel="preconnect" href="https://fonts.googleapis.com">     
		      <div style="text-align: center;">
			  <p style="font-family: 'Dancing Script', cursive; font-weight: 600; display: inline-block; background: linear-gradient(to right, #f00, #ff0, #0f0, #0ff, #00f, #f0f, #f00); -webkit-background-clip: text; -webkit-text-fill-color: transparent; color: transparent; transform: scale(2);">Xiangyue Liu</p>
			</div>
              </p>
              <p>
	     		I'm a first-year Ph.D. student at <a href="https://hkust.edu.hk/">HKUST</a>, advised by Prof. <a href="https://scholar.google.com/citations?user=XhyKVFMAAAAJ&hl=en">Ping Tan</a>. Before that, I was a visiting student (RA) at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, advised by Prof. <a href="https://ericyi.github.io/">Li Yi</a> and Prof. <a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/">Yang Gao</a>. Previously, I completed my Master's from Beihang University.
	      </p>   
		  
		<p>
               		My research focuses on deep learning and computer vision, with a strong emphasis on <strong>3D/4D Editing/ Generation/ Reconstruction</strong> utilizing deep generative models. Early interests were <strong>visual SLAM</strong> and <strong>MVS</strong>.
		</p>
         
              <p style="text-align:center">
		<a href="https://scholar.google.com/citations?user=m3c3jnYAAAAJ&hl=en">Google Scholar</a> &nbsp;&middot&nbsp;
	        <a href="https://github.com/Lxiangyue">Github</a> &nbsp;&middot&nbsp;
		<a href="https://twitter.com/star_chenxi">Twitter</a> &nbsp;&middot&nbsp;
		<a href="mailto:Liuxiangyue@buaa.edu.cn">Email</a>
		<!-- 		<a href="images/scan_code.jpeg">WeChat</a>  -->
		<!--                 <a href="https://www.linkedin.com/in/xiangyue-liu-9007a3190/">LinkedIn</a> &nbsp/&nbsp -->
		<!--                 <a href="data/cv_xiangyue_liu.Paper">CV</a>  -->
              </p>
            </td>
		  
           </tr>
        </tbody></table>


	<table width="93%" align="center" border="0" cellspacing="0" cellpadding="10">
	  <tbody>
	    <tr>
		<td style="padding:10px;width:0%;vertical-align:middle; margin-bottom: 0px">
	        <p><heading>🔥 News</heading></p>
	        <ul>
	          <li><span style="background-color: yellow; color: red; font-weight: bold;">NEW</span> [2024/02] One paper accepted to CVPR 2024 🎉.</li>
	          <li><span style="background-color: yellow; color: red; font-weight: bold;">NEW</span> [2023/09] I start pursuing my Ph.D. degree at HKUST.</li>
	          <li>[2022/06] Two papers accepted to ECCV 2022.</li>
	          <li>[2022/03] One paper accepted to CVPR 2022.</li>
	        </ul>
	      </td>
	    </tr>
	  </tbody>
	</table>
		
	
	      
        <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:0%;vertical-align:middle">
              <heading>📑 Publications</heading>
            </td>		    
          </tr>
        </tbody>
	</table>
        <table style="width:91%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr style="font-size: 0.8em;">
		<td style="padding:20px;width:5%;vertical-align:middle">
		* equal contribution.
		</td>
	</tr>
		
	<tr style="padding-bottom: 50px;">
	  <td style="padding:20px;width:5%;vertical-align:middle">
	    <div class="one">
	      <video width="200" muted autoplay loop>
	        <source src="images/GenN2N_v1.mp4" type="video/mp4">
	      </video>
	    </div>
	  </td>
	  <td width="75%" valign="middle">
	    <div style="height: 60px;"></div>
	    <a href="https://xiangyueliu.github.io/GenN2N/">
	      <papertitle>GenN2N: Generative NeRF2NeRF Translation</papertitle>
	    </a>
	    <font style="line-height:1.5;">
	      <br>
	      <strong>Xiangyue Liu</strong>, Han Xue, Kunming Luo, Ping Tan, Li Yi
	      <br>
	      <gray><em>CVPR 2024</em></gray>&nbsp;
		<a href="https://xiangyueliu.github.io/GenN2N/" class="gray-link">[Project Page]</a>&nbsp;
		<a href="" class="gray-link">[Paper]</a>&nbsp;
		<a href="https://github.com/Lxiangyue/GenN2N" class="gray-link">[Code]</a>
	    </font>

		  
		<font style="font-size: 0.8em; line-height:0.00033em; color: #999999;">
		<br>
		We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, inpainting, etc. GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space. 
		</font>
		  


		  
	    <p></p>
	  </td>
	</tr>

		
	
	<tr style="height: 50px;"></tr>
			
<!-- 	<tr>
	  <td style="padding:20px;width:5%;vertical-align:middle">
	    <div class="one">
	      <video width="200" muted autoplay loop>
	        <source src="images/all-in-one.mp4" type="video/mp4">
	      </video>
	    </div>
	  </td>
	  <td width="75%" valign="middle">
	    <a href="https://xiangyueliu.github.io/Uni3D/">
	      <papertitle>Uni3D: Single Video-based Dynamic 3D Modeling for Humans and Animals with a Unified Template</papertitle>
	    </a>
	    <font style="line-height:1.5;">
	      <br>
	      <strong>Xiangyue Liu</strong>, Weixin Xu, Shuchang Zhou, Li Yi, Yang Gao
	      <br>
	      <em>Under review</em>
	      <br>
	      <a href="https://xiangyueliu.github.io/Uni3D/">Project</a>
	    </font>
	    <p></p>
	  </td>
	</tr>
 -->
		
	
		
	<tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <img src="images/sob.png" alt="PontTuset" width="200" style="border-style: none">
	    </td>
	    <td width="75%" valign="middle">
	      <a href="https://arxiv.org/abs/2207.10395?context=cs" id="Sobolev_INRs">
		<papertitle>Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives</papertitle>
	      </a>
		    <font style="line-height:1.5;">
		      <br>
		      Wentao Yuan, Qingtian Zhu, <strong>Xiangyue Liu</strong>, Yikang Ding, Haotian Zhang, Chi Zhang
		      <br>
			<gray><em>ECCV 2022</em></gray>&nbsp;
			<a href="https://arxiv.org/abs/2207.10395?context=cs" class="gray-link">[Paper]</a>&nbsp;
			<a href="https://github.com/megvii-research/Sobolev_INRs" class="gray-link">[Code]</a>
		    </font>
		    
			<font style="font-size: 0.8em; line-height:0.00033em; color: #999999;">
			<br>
			We propose a training paradigm for Implicit Neural Representations (INRs) that encode image derivatives in addition to image values in the neural network. 
			</font>

	      <p></p>	  
	    </td>
        </tr>

		
		
	<tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <img src="images/horse.png" alt="PontTuset" width="200" style="border-style: none">
	    </td>
	    <td width="75%" valign="middle">
		 <a href="https://arxiv.org/abs/2207.10425" id="KD-MVS">
		<papertitle>KD-MVS: Knowledge Distillation Based Self-supervised Learning for Multi-view Stereo</papertitle>
	      </a>
		    <font style="line-height:1.5;">
		      <br>
		      Yikang Ding, Qingtian Zhu, <strong>Xiangyue Liu</strong>, Wentao Yuan, Haotian Zhang, Chi Zhang
		      <br>
		      <gray><em>ECCV 2022</em></gray>&nbsp;
			<a href="https://arxiv.org/abs/2207.10425" class="gray-link">[Paper]</a>&nbsp;
			<a href="https://github.com/megvii-research/KD-MVS" class="gray-link">[Code]</a>
		    </font>
	
			<font style="font-size: 0.8em; line-height:0.00033em; color: #999999;">
			<br>
			We propose a novel self-supervised training pipeline for MVS based on knowledge distillation, termed KD-MVS, which mainly consists of self-supervised teacher training and distillation-based student training.
			</font>


	      <p></p>	  
	    </td>
        </tr>
		
	<tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <img src="images/transmvsnet.png" alt="PontTuset" width="200" style="border-style: none">
	    </td>
	    <td width="75%" valign="middle">
	      <a href="https://arxiv.org/abs/2111.14600" id="TransMVSNet">
		<papertitle>TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers</papertitle>
	      </a>
		     <font style="line-height:1.5;">
		      <br>
		      Yikang Ding*, Wentao Yuan*, Qingtian Zhu, Haotian Zhang, <strong>Xiangyue Liu</strong>, Yuanjiang Wang, Xiao Liu (* equal contribution)
		      <br>
			    <gray><em>CVPR 2022</em></gray>&nbsp;
			    <a href="https://arxiv.org/abs/2111.14600" class="gray-link">[Paper]</a>&nbsp;
			    <a href="https://github.com/MegviiRobot/TransMVSNet" class="gray-link">[Code]</a>
		     </font>
		
				<font style="font-size: 0.8em; line-height:0.00033em; color: #999999;">
				<br>
				We analogize MVS back to its nature of a feature matching task and therefore propose a powerful Feature Matching Transformer to leverage self- and cross- attention to aggregate long-range context information within and across images.
				</font>

	      <p></p>	  
	    </td>
        </tr>

	<tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <img src="images/icra.png" alt="PontTuset" width="200" style="border-style: none">
	    </td>
	    <td width="75%" valign="middle">
	      <a href="https://ieeexplore.ieee.org/document/9561283" id="RPR">
		<papertitle>Structure Reconstruction Using Ray-Point-Ray Features: Representation and Camera Pose Estimation</papertitle>
	      </a>
		  <font style="line-height:1.5;">
		      <br>
		      Yijia He*, <strong>Xiangyue Liu*</strong>, Xiao Liu, Ji Zhao (* equal contribution)
		      <br>
			 <gray><em>ICRA 2021</em></gray>&nbsp;
			 <a href="https://ieeexplore.ieee.org/document/9561283" class="gray-link">[Paper]</a>
		   </font>
	
			<font style="font-size: 0.8em; line-height:0.00033em; color: #999999;">
			<br>
			Straight line features have been increasingly utilized in visual SLAM and 3D reconstruction systems. The straight lines’ parameterization, parallel constraint, and coplanar constraint are studied in many recent works. In this paper, we explore the novel intersection constraint of straight lines for structure reconstruction. 
			</font>

	      <p></p>	  
	    </td>
        </tr>

<!-- 	<tr style="font-size: 0.8em;">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp* means equal contribution.</tr> -->

<!-- 	<tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <img src="images/iac.png" alt="PontTuset" width="200" style="border-style: none">
	    </td>
	    <td width="75%" valign="middle">
	      <a href="https://iafastro.directory/iac/paper/id/57818/summary/" id="IAC">
		<papertitle>An Optimization Algorithm of Baseline Density Distribution for An Ultra-long Wave Astronomical Observation Array</papertitle>
	      </a>
	      <br>
	      Jingwei Yang, Juntao Pu, Xinyi Fei, <strong>Xiangyue Liu</strong>, Song You, Li Deng
	      <br>
	      <em>International Astronautical Congress (<strong>IAC</strong>), 2020</em>
	      <br>
              <a href="https://iafastro.directory/iac/paper/id/57818/summary/">Paper</a>
	      <p></p>	  
	    </td>
        </tr> -->
        </tbody></table>


	      
<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>AR Project</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	      
	    <tr>
	      <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		  <video  width="200" muted autoplay loop>
		  <source src="images/arcat-ipad-9s.mp4" type="video/mp4">
		  Your browser does not support the video tag.
		  </video>
		</div>
	      </td>
		    <td style="padding:20px;width:75%;vertical-align:middle">
		  <a href="">
		    <papertitle> Reconstruct Dynamic Non-rigid Objects from Monocular RGB Videos </papertitle>
		  </a>
			<font style="line-height:1.5;">
			      <br><strong>Xiangyue Liu</strong> 
			     <br>cats videos -> cat model <br>
			</font>
		</td>
	      </tr> -->

	      
		
<!-- 			    <tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
			          <video  width="200" muted autoplay loop>
			          <source src="images/arwomen-2s.mp4" type="video/mp4">
			          Your browser does not support the video tag.
			          </video>
			        </div>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
		              <br> dancing samba videos -> breakdance <br>
			        </td>
			      </tr>  -->
		
<!--         <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
        <table width="95%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
<!--             <td style="padding:20px;width:100%;vertical-align:middle"> -->
	    <td style="padding:10px;width:0%;vertical-align:middle">
              <heading>🌏 Challenge</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:91%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
	    <td style="padding:20px;width:5%;vertical-align:middle">
	      <img src="images/ijcai.png" alt="PontTuset" width="200" style="border-style: none">
	    </td>
	    <td width="65%" valign="middle">
	      <a href="https://arxiv.org/abs/2010.10957" id="IJACI">
		<papertitle>2nd Place Solution to Instance Segmentation of IJCAI 3D AI Challenge 2020 (2/559)</papertitle>
	      </a>
		    <font style="line-height:1.5;">
		      <br>
		      Kai Jiang*, <strong>Xiangyue Liu*</strong>, Zhang Ju*, Xiang Luo (* equal contribution)
		      <br>
			 <gray><em>IJCAI 2020 workshop</em></gray>&nbsp;
			 <a href="https://arxiv.org/abs/2010.10957" class="gray-link">[Paper]</a>
			</font>
	      <p></p>	  
	    </td>
        </tr>

	      
        <table width="95%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
<!--             <td width="75%" valign="center"> -->
	    <td style="padding:10px;width:0%;vertical-align:middle">
		<p><heading>🏆 Awards</heading></p>
	       <ul>
	            <li>[2021/06] Excellent Graduate in Beijing</li>
	            <li>[2020/08] 2<sup>nd</sup> in IJCAI 3D AI Challenge: Instance Segmentation (2/559) <a href="https://ijcai20.org/w35/">website</a></li>
	            <li>[2016/10] National College Students Innovation and Entrepreneurship Training Program</li>
	       </ul>	
            </td>
          </tr>
        </tbody></table>
	      
        <table width="95%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
<!--             <td width="75%" valign="center"> -->
            <td style="padding:10px;width:0%;vertical-align:middle">
		<p><heading>🎓 Educations</heading></p>
	       <ul>
		    <li>2023.09 - future: Ph.D. in Electronic and Computer Engineering, Hong Kong University of Science and Technology</li>
	            <li>2018.09 - 2021.06: M.Sc. in Software Engineering, Beihang University</li>
	            <li>2014.09 - 2018.06: B.Eng. in Software Engineering, Northeast Normal University</li>
	       </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="95%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
<!--             <td width="75%" valign="center"> -->
	    <td style="padding:10px;width:0%;vertical-align:middle">
		<p><heading>👩🏻‍💻 Internships</heading> </p>
	       <ul>
	            <li>2021.08 - 2023.08: Tsinghua University, IIIS, supervised by Prof. <a href="https://ericyi.github.io/">Li Yi</a>, and Prof. <a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/">Yang Gao</a></li>
	            <li>2022.04 - 2023.08: MEGVII Research, AIC, supervised by Weixin Xu, Yi Yang, and Dr. <a href="http://www.liuxiao.org/">Shuchang Zhou</a></li>
	            <li>2021.06 - 2022.04: MEGVII Research, 3D Vision, supervised by Haotian Zhang, and <a href="http://www.liuxiao.org/">Xiao Liu</a></li>
	            <li>2019.08 - 2020.04: MEGVII Research, SLAM&AR, supervised by Dr. <a href="https://scholar.google.com/citations?hl=en&user=_0lKGnkAAAAJ&view_op=list_works&sortby=pubdate">Yijia He</a>, and <a href="http://www.liuxiao.org/">Xiao Liu</a></li>
	       </ul>		    
            </td>
          </tr>
        </tbody></table>

        <table width="95%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td width="75%" valign="center">
                 <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=593t8ri9kch&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33&amp;s=170" async="async"></script>
            </td>
          </tr>
        </tbody></table>

        <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;color: gray">
		 <em>Last update: Mar. 2024</em>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
